{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21614b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Shape: (307511, 57)\n",
      "\n",
      "TARGET distribution:\n",
      "TARGET\n",
      "0    0.919271\n",
      "1    0.080729\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Train TARGET distribution:\n",
      " TARGET\n",
      "0    0.919271\n",
      "1    0.080729\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test TARGET distribution:\n",
      " TARGET\n",
      "0    0.919272\n",
      "1    0.080728\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Features scaled successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bottleneck (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,224</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m7,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bottleneck (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m)             │         \u001b[38;5;34m7,224\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,288</span> (137.84 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m35,288\u001b[0m (137.84 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,288</span> (137.84 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m35,288\u001b[0m (137.84 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m860/884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1934"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "DATA_PATH = '../application_train_FinalEDA.csv'\n",
    "RISK_PROFILE_TRAIN_OUT = \"risk_profiled_autoencoder_train.csv\"\n",
    "AE_TEST_FEATURES_OUT = \"autoencoder_test_features.csv\"\n",
    "SUMMARY_OUT = \"evaluation_results_summary.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(DATA_PATH)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Shape:\", df1.shape)\n",
    "print(\"\\nTARGET distribution:\")\n",
    "print(df1['TARGET'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "X = df1.drop(columns=['TARGET'])\n",
    "y = df1['TARGET']\n",
    "\n",
    "# TRAIN-TEST SPLIT (STRATIFIED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\nTrain TARGET distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest TARGET distribution:\\n\", y_test.value_counts(normalize=True))\n",
    "\n",
    "# FEATURE SCALING\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeatures scaled successfully!\")\n",
    "\n",
    "# DEFINING AND TRAINING AUTOENCODER\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "# Encoder-Decoder architecture\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "bottleneck = Dense(encoding_dim, activation='relu', name=\"bottleneck\")(encoded)\n",
    "decoded = Dense(64, activation='relu')(bottleneck)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "# Training autoencoder on NON-DEFAULTERS only (anomaly-detection style)\n",
    "X_train_ae = X_train_scaled[y_train.values == 0]\n",
    "if X_train_ae.shape[0] < 10:\n",
    "    print(\"Warning: very few non-defaulters in training set; autoencoder training may be unstable.\")\n",
    "\n",
    "\n",
    "if (y_test.values == 0).sum() > 0:\n",
    "    X_val_ae = X_test_scaled[y_test.values == 0]\n",
    "else:\n",
    "    X_val_ae = X_test_scaled  # fallback\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train_ae, X_train_ae,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val_ae, X_val_ae),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Autoencoder Training Loss (MSE)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# EXTRACTING ENCODED (LATENT) FEATURES\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "encoded_train_df = pd.DataFrame(X_train_encoded, columns=[\n",
    "                                f\"encoded_{i}\" for i in range(encoding_dim)])\n",
    "encoded_test_df = pd.DataFrame(X_test_encoded, columns=[\n",
    "                               f\"encoded_{i}\" for i in range(encoding_dim)])\n",
    "\n",
    "encoded_train_df['TARGET'] = y_train.reset_index(drop=True)\n",
    "encoded_test_df['TARGET'] = y_test.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nEncoded features created successfully!\")\n",
    "print(\"Train encoded shape:\", encoded_train_df.shape)\n",
    "print(\"Test encoded shape:\", encoded_test_df.shape)\n",
    "\n",
    "# K-MEANS CLUSTERING (RISK PROFILING)\n",
    "X_encoded = encoded_train_df.drop(columns=['TARGET'])\n",
    "y_encoded = encoded_train_df['TARGET']\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE)\n",
    "encoded_train_df['Cluster'] = kmeans.fit_predict(X_encoded)\n",
    "\n",
    "cluster_summary = (\n",
    "    encoded_train_df.groupby('Cluster')['TARGET']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(by='TARGET')\n",
    ")\n",
    "cluster_summary.columns = ['Cluster', 'Average_Default_Rate']\n",
    "\n",
    "print(\"\\n Default Rate by Cluster:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# LABEL CLUSTERS AS RISK LEVELS\n",
    "risk_map = cluster_summary.sort_values(\n",
    "    'Average_Default_Rate').reset_index(drop=True)\n",
    "risk_map['Risk_Level'] = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "risk_label_dict = dict(zip(risk_map['Cluster'], risk_map['Risk_Level']))\n",
    "\n",
    "encoded_train_df['Risk_Level'] = encoded_train_df['Cluster'].map(\n",
    "    risk_label_dict)\n",
    "\n",
    "print(\"\\n Cluster → Risk Level mapping:\")\n",
    "print(risk_label_dict)\n",
    "print(\"\\n Risk Level Distribution:\")\n",
    "print(encoded_train_df['Risk_Level'].value_counts())\n",
    "\n",
    "# VISUALIZE CLUSTERS USING PCA\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "pca_result = pca.fit_transform(X_encoded)\n",
    "\n",
    "encoded_train_df['PCA1'] = pca_result[:, 0]\n",
    "encoded_train_df['PCA2'] = pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for level in encoded_train_df['Risk_Level'].unique():\n",
    "    subset = encoded_train_df[encoded_train_df['Risk_Level'] == level]\n",
    "    plt.scatter(subset['PCA1'], subset['PCA2'], label=level, alpha=0.6)\n",
    "plt.title(\"PCA Projection of Autoencoder Risk Clusters\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# SAVE THE RISK-PROFILED DATASET\n",
    "encoded_train_df.to_csv(RISK_PROFILE_TRAIN_OUT, index=False)\n",
    "encoded_test_df.to_csv(AE_TEST_FEATURES_OUT, index=False)\n",
    "\n",
    "print(\"\\nSaved successfully:\")\n",
    "print(\"→\", RISK_PROFILE_TRAIN_OUT)\n",
    "print(\"→\", AE_TEST_FEATURES_OUT)\n",
    "\n",
    "# EVALUATION: Reconstruction metrics, distribution, ROC-AUC, thresholding, confusion matrix\n",
    "\n",
    "# Reconstruct test set using autoencoder\n",
    "X_test_recon = autoencoder.predict(X_test_scaled)\n",
    "# Per-sample reconstruction error (MSE per sample)\n",
    "test_errors = np.mean(np.square(X_test_scaled - X_test_recon), axis=1)\n",
    "\n",
    "# Reconstruction on training non-defaulters used for AE training (for thresholding & distribution)\n",
    "X_train_ae_recon = autoencoder.predict(X_train_ae)\n",
    "train_errors = np.mean(np.square(X_train_ae - X_train_ae_recon), axis=1)\n",
    "\n",
    "# Aggregated reconstruction metrics (on full test set)\n",
    "recon_mse = mean_squared_error(X_test_scaled, X_test_recon)\n",
    "recon_mae = mean_absolute_error(X_test_scaled, X_test_recon)\n",
    "\n",
    "print(\"\\n--- Autoencoder Reconstruction Metrics (on test set) ---\")\n",
    "print(f\"Reconstruction MSE (test, aggregated): {recon_mse:.6f}\")\n",
    "print(f\"Reconstruction MAE (test, aggregated): {recon_mae:.6f}\")\n",
    "\n",
    "# Plot reconstruction error distribution (train non-defaulters)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(train_errors, bins=80, alpha=0.6)\n",
    "plt.title('Reconstruction error distribution (train non-defaulters)')\n",
    "plt.xlabel('Reconstruction error (MSE per sample)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# reconstruction error histogram (overlay)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(train_errors, bins=80, alpha=0.5, label='train non-defaulters')\n",
    "plt.hist(test_errors, bins=80, alpha=0.5, label='test (all)')\n",
    "plt.title('Reconstruction error: train (non-defaulters) vs test')\n",
    "plt.xlabel('Reconstruction error (MSE per sample)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC using reconstruction error as anomaly score\n",
    "roc_auc = roc_auc_score(y_test, test_errors)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_errors)\n",
    "print(f\"\\nROC-AUC (autoencoder reconstruction error): {roc_auc:.4f}\")\n",
    "\n",
    "# Chose threshold via Youden's J statistic\n",
    "j_scores = tpr - fpr\n",
    "j_idx = np.argmax(j_scores)\n",
    "opt_threshold = thresholds[j_idx]\n",
    "# 95th percentile of training non-defaulters\n",
    "pct_threshold = np.percentile(train_errors, 95)\n",
    "\n",
    "print(f\"Optimal threshold (Youden): {opt_threshold:.6f}\")\n",
    "print(f\"95th percentile threshold (train errors): {pct_threshold:.6f}\")\n",
    "\n",
    "# Binary predictions using chosen threshold (Youden)\n",
    "y_pred_ae = (test_errors >= opt_threshold).astype(int)\n",
    "\n",
    "prec_ae = precision_score(y_test, y_pred_ae, zero_division=0)\n",
    "rec_ae = recall_score(y_test, y_pred_ae, zero_division=0)\n",
    "f1_ae = f1_score(y_test, y_pred_ae, zero_division=0)\n",
    "cm_ae = confusion_matrix(y_test, y_pred_ae)\n",
    "\n",
    "print(\"\\n--- Autoencoder Classification-style Metrics (using Youden threshold) ---\")\n",
    "print(\"Confusion matrix:\\n\", cm_ae)\n",
    "print(f\"Precision: {prec_ae:.4f}, Recall: {rec_ae:.4f}, F1: {f1_ae:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm_ae, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix (Autoencoder - Youden threshold)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "for i in range(cm_ae.shape[0]):\n",
    "    for j in range(cm_ae.shape[1]):\n",
    "        plt.text(j, i, cm_ae[i, j], ha=\"center\", va=\"center\", fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', linewidth=0.8)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Autoencoder reconstruction error)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(\n",
    "    y_test, test_errors)\n",
    "ap_score = average_precision_score(y_test, test_errors)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall_vals, precision_vals, label=f\"AP = {ap_score:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve (Autoencoder)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# COMPARISON WITH SUPERVISED MODELS\n",
    "print(\"\\nTraining supervised classifiers for comparison...\")\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Probabilities and preds on test\n",
    "y_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "y_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "auc_lr = roc_auc_score(y_test, y_proba_lr)\n",
    "prec_lr = precision_score(y_test, y_pred_lr, zero_division=0)\n",
    "rec_lr = recall_score(y_test, y_pred_lr, zero_division=0)\n",
    "f1_lr = f1_score(y_test, y_pred_lr, zero_division=0)\n",
    "\n",
    "auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "prec_rf = precision_score(y_test, y_pred_rf, zero_division=0)\n",
    "rec_rf = recall_score(y_test, y_pred_rf, zero_division=0)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, zero_division=0)\n",
    "\n",
    "print('\\n--- Supervised models metrics (on same test set) ---')\n",
    "print('Logistic Regression: AUC={:.4f}, Precision={:.4f}, Recall={:.4f}, F1={:.4f}'.format(\n",
    "    auc_lr, prec_lr, rec_lr, f1_lr))\n",
    "print('Random Forest:        AUC={:.4f}, Precision={:.4f}, Recall={:.4f}, F1={:.4f}'.format(\n",
    "    auc_rf, prec_rf, rec_rf, f1_rf))\n",
    "\n",
    "# Save summary table\n",
    "results = pd.DataFrame({\n",
    "    'model': ['Autoencoder (AE error->AUC)', 'Autoencoder (thresholded)', 'LogisticRegression', 'RandomForest'],\n",
    "    'roc_auc': [roc_auc, roc_auc, auc_lr, auc_rf],\n",
    "    'precision': [np.nan, prec_ae, prec_lr, prec_rf],\n",
    "    'recall': [np.nan, rec_ae, rec_lr, rec_rf],\n",
    "    'f1': [np.nan, f1_ae, f1_lr, f1_rf]\n",
    "})\n",
    "results.to_csv(SUMMARY_OUT, index=False)\n",
    "print(f\"\\nAll summary results saved to {SUMMARY_OUT}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Final summary ---\")\n",
    "print(results)\n",
    "\n",
    "\n",
    "test_results_df = pd.DataFrame({\n",
    "    'reconstruction_error': test_errors,\n",
    "    'y_true': y_test.reset_index(drop=True),\n",
    "    'y_pred_ae_youden': y_pred_ae\n",
    "})\n",
    "test_results_df.to_csv(\"autoencoder_test_errors_with_labels.csv\", index=False)\n",
    "print(\"Saved test errors to autoencoder_test_errors_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4c399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
